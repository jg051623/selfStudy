{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepLearning Lab-02",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOsTqtjGnTxecBDXjCJHJ5u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jg051623/selfStudy/blob/main/DeepLearning_Lab_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wCpdmJcVu5E"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_data = [1,2,3,4,5]\n",
        "y_data = [1,2,3,4,5]\n",
        "\n",
        "W = tf.Variable(2.9)\n",
        "b = tf.Variable(0.5)\n",
        "\n",
        "## tf.Variable : mutable value / tf.constant : immutable\n",
        "\n",
        "hypothesis = W * x_data + b\n",
        "\n",
        "coxt = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
        "\n",
        "## tf.reduce_mean() : 차원 즉, rank가 하나 줄어들면서 mean 을 구한다.\n",
        "#  v = [1.,2.,3.,4.]  rank : 1\n",
        "# tf.reduce_mean(v) # rank : 0\n"
      ],
      "metadata": {
        "id": "5znDSmXSVy0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Gradient descent : cost(W,b)가 minimize 되는 W,b를 찾는 과정\n",
        "\n",
        "learning rate = 0.01\n",
        "\n",
        "with tf.GradientTabe() as tape :\n",
        "  hypothesis = W * x_data + b\n",
        "  cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
        "\n",
        "# W,b(변수)에 대한 정보를 tape에 기억하고 이후에 tape에 gradient 메소드를 호출해서 경사도값 즉 미분값을 구함\n",
        "# W,b에 대한 개별 미분값(=기울기) 을 tuple로 반환\n",
        "\n",
        "W_grad, b_grad = tape.gradient(cost, [W,b])\n",
        "\n",
        "# gradient function : cost function에서 W,b에 대한 미분값을 W_grad, b_grad에 assign\n",
        "\n",
        "#A.assign_sub(B)\n",
        "# A = A - B \n",
        "# A -= B\n",
        "\n",
        "W.assign_sub(learning_rate * W_grad)\n",
        "b.assign_sub(learning_rate * b_grad)\n",
        "\n",
        "## learning rate : 기울기를 얼마만큼 반영할 것인지 결정 / learning rate 가 작다면 조금만 반영하는 것이기 때문에 조금씩 변화가 일어남\n",
        "# 위의 과정을 끝내면 W,b 가 update\n"
      ],
      "metadata": {
        "id": "7p_pPaRcWw_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Full code\n",
        "x_data = [1,2,3,4,5]\n",
        "y_data = [1,2,3,4,5]\n",
        "\n",
        "W = tf.Variable(2.9)\n",
        "b = tf.Variable(0.5)\n",
        "\n",
        "learning_rate = 0.01\n",
        "\n",
        "for i in range(100+1) :\n",
        "  with tf.GradientTape() as tape :\n",
        "    hypothesis = W * x_data + b\n",
        "    cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
        "\n",
        "  W_grad,b_grad = tape.gradient(cost,[W,b])\n",
        "  W.assign_sub(learning_rate * W_grad)\n",
        "  b.assign_sub(learning_rate * b_grad)\n",
        "\n",
        "  if i % 10 == 0 :\n",
        "    print(\"{:5}|{:10.4f}|{:10.4}|{:10.6f}\".format(i,W.numpy(),b.numpy(),cost))\n",
        "\n",
        "#https://firedino.tistory.com/56    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDxBcERmYSrZ",
        "outputId": "b0b695bd-af39-4bc9-9c30-46de5e1bd3ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    0|    2.4520|     0.376| 45.660004\n",
            "   10|    1.1036|  0.003398|  0.206336\n",
            "   20|    1.0128|  -0.02091|  0.001026\n",
            "   30|    1.0065|  -0.02184|  0.000093\n",
            "   40|    1.0059|  -0.02123|  0.000083\n",
            "   50|    1.0057|  -0.02053|  0.000077\n",
            "   60|    1.0055|  -0.01984|  0.000072\n",
            "   70|    1.0053|  -0.01918|  0.000067\n",
            "   80|    1.0051|  -0.01854|  0.000063\n",
            "   90|    1.0050|  -0.01793|  0.000059\n",
            "  100|    1.0048|  -0.01733|  0.000055\n"
          ]
        }
      ]
    }
  ]
}