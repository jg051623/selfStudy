{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepLearning_Lab_07-1_2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNZKE2mUBgo9Gt412y297Rx"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "v6gaQ50cWRuX"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# learning rate decay\n",
        "\n",
        "# initial_learning_rate = 0.1\n",
        "# lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "#     initial_learning_rate,\n",
        "#     decay_steps=100000,\n",
        "#     decay_rate=0.96,\n",
        "#     staircase=True)\n",
        "\n",
        "# optimizer = keras.optimizers.RMSprop(learning_rate=lr_schedule)\n",
        "\n",
        "# ExponentialDecay, PiecewiseConstantDecay, PolynomialDecay and InverseTimeDecay\n",
        "# 여기에서 staircase는 learning rate를 계단식으로 감소시켜준다. 즉 decay_steps에 도달하기 전에는 learning rate를 유지시키다가, \n",
        "# decay_steps가 넘어가면 그 때 학습률을 감소시키는 것이다.\n",
        "\n",
        "\n",
        "# def decayed_learning_rate(step):\n",
        "#   return initial_learning_rate * decay_rate ^ (step / decay_steps)\n",
        "# if staircase = True : step/decay_steps : integer division"
      ],
      "metadata": {
        "id": "QQCr-CufWWp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def exponential_decay(epoch) :\n",
        "  initial_rate = 0.01\n",
        "  k = 0.96\n",
        "  exp_rate = initial_rate * np.exp(-k * t)"
      ],
      "metadata": {
        "id": "aEEd4rYTXvmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Feature Scaling\n",
        "\n",
        "# sd = sqrt(np.sum((data - np.mean(data))^2) / np.count/(data))\n",
        "# standardization = (data - np.mean(data)) / sd\n",
        "\n",
        "# normalization = (data - np.min(data, axis = 0)) / (np.max(data, 0) - np.min(data, 0))"
      ],
      "metadata": {
        "id": "9Z_AXP_eaFly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
        "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
        "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
        "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
        "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
        "               [819, 823, 1198100, 816, 820.450012],\n",
        "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
        "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])\n",
        "\n",
        "## 성격이 다른 데이터 몇 개가 보임 ex) 908100\n",
        "\n",
        "\n",
        "def normalization(data) :\n",
        "  numerator = data - np.min(data,0)\n",
        "  denominator = np.max(data,0) - np.min(data,0)\n",
        "  return numerator / denominator\n",
        "\n",
        "\n",
        "xy = normalization(xy)\n",
        "xy = xy.astype(np.float32)\n",
        "\n",
        "x_train = xy[:, 0:-1]\n",
        "y_train = xy[:,[-1]]"
      ],
      "metadata": {
        "id": "Z_PgNi69cb1x"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((x_train,y_train)).batch(len(x_train))\n",
        "\n",
        "W = tf.Variable(tf.random.normal([4,1]),dtype=tf.float32)\n",
        "b = tf.Variable(tf.random.normal([1]),dtype=tf.float32)"
      ],
      "metadata": {
        "id": "Oz52Wu2_ZE1k"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linearReg_fn(features) :\n",
        "  hypothesis = tf.matmul(features, W) + b\n",
        "  return hypothesis\n",
        "\n",
        "def L2_loss(loss, beta = 0.01) :\n",
        "  W_reg = tf.nn.l2_loss(W)  #output = sum(t ** 2) / 2\n",
        "  loss = tf.reduce_mean(loss + W_reg * beta)\n",
        "  return loss \n",
        "\n",
        "def loss_fn(hypothesis, labels, flag = False) :\n",
        "  cost = tf.reduce_mean(tf.square(hypothesis - labels))\n",
        "  if(flag) :\n",
        "    cost = L2_loss(cost)\n",
        "  return cost\n"
      ],
      "metadata": {
        "id": "3GuHlpqvZizh"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "is_decay = True\n",
        "starter_learning_rate = 0.1\n",
        "\n",
        "if(is_decay) :\n",
        "  lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    starter_learning_rate,\n",
        "    decay_steps=50,\n",
        "    decay_rate=0.96,\n",
        "    staircase=True)\n",
        "  optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr_schedule)\n",
        "else :\n",
        "  optimizer = tf.keras.optimizers.SGD(learning_rate = starter_learning_rate)"
      ],
      "metadata": {
        "id": "Bb-7qKO1cRtW"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grad(features, labels, l2_flag) :\n",
        "  with tf.GradientTape() as tape :\n",
        "    loss_value = loss_fn(linearReg_fn(features),labels, l2_flag)\n",
        "  return tape.gradient(loss_value, [W,b]), loss_value"
      ],
      "metadata": {
        "id": "qwFR6NjIdaSn"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for step in range(151) :\n",
        "  for features, labels in dataset :\n",
        "    features = tf.cast(features, tf.float32)\n",
        "    labels = tf.cast(labels, tf.float32)\n",
        "    grads, loss_value = grad(features, labels, False)\n",
        "    optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b]))\n",
        "    current_learning_rate = optimizer._decayed_lr(tf.float32).numpy()\n",
        "    if step % 10 == 0 :\n",
        "      print(\"Iter : {}, Loss : {:.4f}, lr : {}\".format(step,loss_value,current_learning_rate))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHa2NLisfIz-",
        "outputId": "a1662caf-c599-45d4-c13b-11d146690ca1"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter : 0, Loss : 3.6949, lr : 0.10000000149011612\n",
            "Iter : 10, Loss : 0.3817, lr : 0.10000000149011612\n",
            "Iter : 20, Loss : 0.1694, lr : 0.10000000149011612\n",
            "Iter : 30, Loss : 0.0543, lr : 0.10000000149011612\n",
            "Iter : 40, Loss : 0.0110, lr : 0.10000000149011612\n",
            "Iter : 50, Loss : 0.0574, lr : 0.09600000083446503\n",
            "Iter : 60, Loss : 0.0038, lr : 0.09600000083446503\n",
            "Iter : 70, Loss : 0.0335, lr : 0.09600000083446503\n",
            "Iter : 80, Loss : 0.0269, lr : 0.09600000083446503\n",
            "Iter : 90, Loss : 0.0147, lr : 0.09600000083446503\n",
            "Iter : 100, Loss : 0.0216, lr : 0.09216000139713287\n",
            "Iter : 110, Loss : 0.0292, lr : 0.09216000139713287\n",
            "Iter : 120, Loss : 0.0191, lr : 0.09216000139713287\n",
            "Iter : 130, Loss : 0.0195, lr : 0.09216000139713287\n",
            "Iter : 140, Loss : 0.0226, lr : 0.09216000139713287\n",
            "Iter : 150, Loss : 0.0221, lr : 0.0884735956788063\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.cast(x_train, tf.float32)\n",
        "\n",
        "# tf.cast 함수는 텐서를 새로운 자료형으로 변환합니다.\n",
        "# Boolean형태인 경우 True이면 1, False이면 0을 출력한다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZEz16mqfx49",
        "outputId": "111f4dcc-5cde-488a-bc19-d943bb7a746f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(8, 4), dtype=float32, numpy=\n",
              "array([[8.28660e+02, 8.33450e+02, 9.08100e+05, 8.28350e+02],\n",
              "       [8.23020e+02, 8.28070e+02, 1.82810e+06, 8.21655e+02],\n",
              "       [8.19930e+02, 8.24400e+02, 1.43810e+06, 8.18980e+02],\n",
              "       [8.16000e+02, 8.20959e+02, 1.00810e+06, 8.15490e+02],\n",
              "       [8.19360e+02, 8.23000e+02, 1.18810e+06, 8.18470e+02],\n",
              "       [8.19000e+02, 8.23000e+02, 1.19810e+06, 8.16000e+02],\n",
              "       [8.11700e+02, 8.15250e+02, 1.09810e+06, 8.09780e+02],\n",
              "       [8.09510e+02, 8.16660e+02, 1.39810e+06, 8.04540e+02]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# current_learning_rate = optimizer._decayed_lr(tf.float32)\n",
        "# print(current_learning_rate)\n",
        "\n",
        "# To print current learning rate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ssIy297jMvI",
        "outputId": "81263bbd-377a-47b7-8835-105585e53987"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(0.088473596, shape=(), dtype=float32)\n"
          ]
        }
      ]
    }
  ]
}