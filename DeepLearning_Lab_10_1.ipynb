{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepLearning_Lab_10-1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP62gY67DhZUdRyDgocHvHK"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cWC9ObCvQwJw"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import mnist "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# image normalization \n",
        "def normalize(train_data, test_data) :\n",
        "  train_data = train_data.astype(np.float32) / 255.0\n",
        "  test_data = test_data.astype(np.float32) / 255.0\n",
        "  return train_data, test_data\n",
        "\n",
        "# load mnist with preprocessing\n",
        "def load_mnist() : \n",
        "  (train_data,train_labels), (test_data, test_labels) = mnist.load_data()\n",
        "\n",
        "  # in tensorflow input shape : (batch_size, height, width, channel)\n",
        "  train_data = np.expand_dims(train_data, axis = -1)  # (N,28,28) -> (N, 28, 28, 1) \n",
        "  test_data = np.expand_dims(test_data, axis = -1)  # (N,28,28) -> (N, 28, 28, 1) \n",
        "\n",
        "  train_data, test_data = normalize(train_data, test_data)  # 0~ 255 > 0~1\n",
        "\n",
        "  # label preprocessing\n",
        "  train_labels = to_categorical(train_labels, 10)  # (n,) -> (n,10) / one-hot encoding / 10 : class 개수\n",
        "  test_labels = to_categorical(test_labels, 10) \n",
        "\n",
        "  return train_data, train_labels, test_data, test_labels"
      ],
      "metadata": {
        "id": "7R_dOXzLQ3Y8"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create network\n",
        "\n",
        "def flatten() :\n",
        "  return tf.keras.layers.Flatten()\n",
        "\n",
        "# To make Fully connected layer\n",
        "def dense(channel,weight_init) :\n",
        "  return tf.keras.layers.Dense(units = channel, use_bias = True, kernel_initializer = weight_init) # units : output으로 나가는 channel 개수 use_bias : bias 사용 여부 kernel_initializer : weight initializer\n",
        "\n",
        "def relu() :\n",
        "  return tf.keras.layers.Activation(tf.keras.activations.relu)\n",
        "\n",
        "# class로 모델 만들때 주의할 점 : tf.keras.Model 상속해야함\n",
        "# label_dim : 여기서는 10을 의미 / output 개수\n",
        "# weight_init = tf.keras.initializers.RandomNormal() : N(0,1) 정규분포에서 랜덤한 weight 설정\n",
        "class create_model(tf.keras.Model) : \n",
        "  def __init__(self, label_dim) :\n",
        "    super(create_model, self).__init__()\n",
        "    weight_init = tf.keras.initializers.RandomNormal()\n",
        "    self.model = tf.keras.Sequential()   # layer을 층층이 쌓아나가는 것을 list에 계속 더해준다고 할 수 있음 / Sequential : list 자료 구조 type\n",
        "\n",
        "    self.model.add(flatten())   # (N,28,28,1) -> (n, 784)  / fully connected layer을 이용하기 때문에 flatten 시킴 / CNN 이라면 필요 없음\n",
        "\n",
        "    for i in range(2) : \n",
        "      # (N,784) > (N,256) > (N,256)\n",
        "      self.model.add(dense(256, weight_init))\n",
        "      self.model.add(relu())\n",
        "\n",
        "    self.model.add(dense(label_dim,weight_init))  #(N,256) -> (N,10)\n",
        "  \n",
        "  def call(self, x, training=None, mask=None) :\n",
        "    x = self.model(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "vPr6gnAhRP1v"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  ## class method 싫은 경우\n",
        "\n",
        "#  def create_model(label_dim) :\n",
        "#    weight_init = tf.keras.initializers.RandomNormal()\n",
        "\n",
        "#    model = tf.keras.Sequential()\n",
        "#    model.add(flatten())\n",
        "\n",
        "#    for i in range(2) :\n",
        "#      model.add(dense(256, weight_init))\n",
        "#      model.add(relu())\n",
        "\n",
        "#     model.add(dense(label_dim, weight_init)) \n",
        "\n",
        "#     return model"
      ],
      "metadata": {
        "id": "2t1vsduRT70R"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss\n",
        "\n",
        "def loss_fn(model, images, labels) : \n",
        "  logits = model(images, training = True) \n",
        "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels))\n",
        "  return loss  \n",
        "\n",
        "def accuracy_fn(model, images, labels) : \n",
        "  logits = model(images, training = False)\n",
        "  prediction = tf.equal(tf.argmax(logits, -1), tf.argmax(labels, -1))  # logits, labels : (batchsize, label_dim)\n",
        "  accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))  # T,F,T > 1,0,1\n",
        "  return accuracy\n",
        "\n",
        "def grad(model, images, labels) :\n",
        "  with tf.GradientTape() as tape :\n",
        "    loss = loss_fn(model, images, labels)\n",
        "  return tape.gradient(loss, model.variables)"
      ],
      "metadata": {
        "id": "pR5PMHLndf8C"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## set hyperparameter\n",
        "\n",
        "# dataset\n",
        "\n",
        "train_x, train_y, test_x, test_y = load_mnist()\n",
        "\n",
        "# hyperparameter\n",
        "\n",
        "learning_rate = 0.001\n",
        "batch_size = 128\n",
        "\n",
        "training_epochs = 1\n",
        "training_iterations = len(train_x) // batch_size\n",
        "\n",
        "label_dim = 10\n",
        "\n",
        "\n",
        "## graph input using dataset API\n",
        "\n",
        "## buffer_size : 주어진 input 개수보다 크게..  크다면 random 하게 shuffling\n",
        "## prefetch : network가 batch size만큼 학습하고 있을 때 미리 메모리에 batch size 만큼 올려놔라\n",
        "# batch : batch size 만큼 network에 던져준다\n",
        "# repeat() : 반복 \n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_x,train_y)).shuffle(buffer_size = 100000).prefetch(buffer_size = batch_size).batch(batch_size).repeat()\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_x,test_y)).shuffle(buffer_size = 100000).prefetch(buffer_size = len(test_x)).batch(len(test_x)).repeat()\n"
      ],
      "metadata": {
        "id": "MZZaI0vZgthp"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Model \n",
        "\n",
        "# dataset Iterator\n",
        "train_iterator = iter(train_dataset)\n",
        "test_iterator = iter(test_dataset)\n",
        "\n",
        "# model\n",
        "network = create_model(label_dim)\n",
        "\n",
        "# training\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)"
      ],
      "metadata": {
        "id": "Xr_VeuXchrK1"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = tf.train.Checkpoint(dnn=network)  # 학습이 끊겼을 때 다시 재학습을 이루어내고 싶을때.. 변경이 되었던 weight들을 불러냄 / test data 돌릴때\n",
        "\n",
        "for epoch in range(training_epochs) :\n",
        "  for idx in range(1, training_iterations) : \n",
        "    train_input, train_label = train_iterator.get_next()\n",
        "    test_input, test_label = test_iterator.get_next()\n",
        "\n",
        "    grads = grad(network, train_input, train_label)\n",
        "    optimizer.apply_gradients(grads_and_vars = zip(grads, network.variables))\n",
        "\n",
        "    train_loss = loss_fn(network, train_input, train_label)\n",
        "    train_accuracy = accuracy_fn(network, train_input, train_label)\n",
        "\n",
        "    test_loss = loss_fn(network, test_input, test_label)\n",
        "    test_accuracy = accuracy_fn(network, test_input, test_label)\n",
        "\n",
        "    print(\"Epoch : {} train_loss : {:.4f}, train_accuracy : {:.4f}, test_accuracy : {:.4f}\".format(epoch, train_loss, train_accuracy, test_accuracy))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "id": "RZRdjG0CjEf3",
        "outputId": "a433f915-817f-46cf-a336-a4730206b0b0"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 0 train_loss : 0.1574, train_accuracy : 0.9609, test_accuracy : 0.9486\n",
            "Epoch : 0 train_loss : 0.1776, train_accuracy : 0.9453, test_accuracy : 0.9447\n",
            "Epoch : 0 train_loss : 0.1456, train_accuracy : 0.9297, test_accuracy : 0.9414\n",
            "Epoch : 0 train_loss : 0.1028, train_accuracy : 0.9688, test_accuracy : 0.9404\n",
            "Epoch : 0 train_loss : 0.0780, train_accuracy : 0.9844, test_accuracy : 0.9439\n",
            "Epoch : 0 train_loss : 0.2042, train_accuracy : 0.9375, test_accuracy : 0.9493\n",
            "Epoch : 0 train_loss : 0.1182, train_accuracy : 0.9688, test_accuracy : 0.9547\n",
            "Epoch : 0 train_loss : 0.1032, train_accuracy : 0.9609, test_accuracy : 0.9562\n",
            "Epoch : 0 train_loss : 0.2080, train_accuracy : 0.9297, test_accuracy : 0.9575\n",
            "Epoch : 0 train_loss : 0.1731, train_accuracy : 0.9531, test_accuracy : 0.9579\n",
            "Epoch : 0 train_loss : 0.1663, train_accuracy : 0.9609, test_accuracy : 0.9562\n",
            "Epoch : 0 train_loss : 0.1201, train_accuracy : 0.9609, test_accuracy : 0.9546\n",
            "Epoch : 0 train_loss : 0.2129, train_accuracy : 0.9297, test_accuracy : 0.9546\n",
            "Epoch : 0 train_loss : 0.1596, train_accuracy : 0.9531, test_accuracy : 0.9543\n",
            "Epoch : 0 train_loss : 0.0926, train_accuracy : 0.9844, test_accuracy : 0.9555\n",
            "Epoch : 0 train_loss : 0.1368, train_accuracy : 0.9531, test_accuracy : 0.9550\n",
            "Epoch : 0 train_loss : 0.1445, train_accuracy : 0.9609, test_accuracy : 0.9560\n",
            "Epoch : 0 train_loss : 0.1370, train_accuracy : 0.9531, test_accuracy : 0.9568\n",
            "Epoch : 0 train_loss : 0.2223, train_accuracy : 0.9453, test_accuracy : 0.9568\n",
            "Epoch : 0 train_loss : 0.1482, train_accuracy : 0.9531, test_accuracy : 0.9558\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-06e433e3d100>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch : {} train_loss : {:.4f}, train_accuracy : {:.4f}, test_accuracy : {:.4f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_input.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hX70KgDzkbeI",
        "outputId": "9ddb120b-571f-42f8-9d52-4dc965118b8e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([128, 28, 28, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_iterations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDI3ysQAoGLo",
        "outputId": "b837dc3d-b470-4751-dfd6-3ccd05fd7c51"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "468"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kL64DypapaIP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}